<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>VGG-19 Model and Analysis</title>
  <!-- CSS only -->
<!--Boot Strap-->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">

<!--Glyphicons-->
<link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">



<!-- Fonts from google -->
<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Bungee+Inline&family=Bungee+Shade&family=Chathura:wght@800&family=Girassol&family=Karantina&family=Niconne&family=Odibee+Sans&family=Romanesco&display=swap" rel="stylesheet"></head>

<!-- Our CSS -->
<link rel="stylesheet" type="text/css" href="/static/css/style.css">

<!-- JQuery from google CDN -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

</head>

<body class="border border-1 border-dark">

<section class="border border-1 border-dark">
    <div class="container-fluid">
        <div class="row"> 
            <div class="col-12 mt-4">
                <div class="h2 text-center">
                    <p class="mb-5 a-h1">Face Mask Detector using TensorFlow, VGG-19 and OpenCV2</p>                  
                </div>
                <div>
                    <blockquote class="blockquote text-center" cite="https://machinelearningmastery.com/what-is-computer-vision/">
                        <p class="mb-0 lead">Computer vision is the automated extraction of information from images. Information can mean anything from <em>3D models, camera position, object detection and recognition</em>  to <em>grouping and searching image content</em>.</p>
                        <footer class="blockquote-footer"> <cite href="https://machinelearningmastery.com/what-is-computer-vision/" title="Source Title">Page ix, Programming Computer Vision with Python, 2012</cite></footer>
                      </blockquote>
                      <h4 class="a-h1">Introduction to Computer Vision</h4>
                    <div class="mt-3">
                        <h6 class="text-secondary">Why VGG-16?</h6>
                        <p>We tested our dataset with two other TensorFlow Keras models and found that VGG-16 had better value loss over training loss. </p>
                    <h6 class="text-secondary">Objective: </h6>
                      <ul>
                        <li>Detect face masks in images selected</li>
                        <li>Detect face masks in images uploaded</li>
                        <li>Detect face masks in real-time video streams</li>
                      </ul>
                    </div>
                </div>
                <div class="row">
                    <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 col-xl-6">
                        <div class="text-center">
                            <h5 class="text-center text-secondary">VGG-19</h5>
                            <img src="./static/Images/vgg16.png" class="image-fluid w-100" height="330px">
                        </div>
                    </div>
                    <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 col-xl-6">
                        <h5 class="text-center text-secondary">Convolutional Network for Classification and Detection</h5>
                        <div class="mt-4">
                            <p><strong class="text-secondary">Model: </strong>Very Deep Convolutional Networks for Large-Scale Image Recognition proposed by K. Simonyan and A. Zisserman from the University of Oxford</p>
                            <p><strong class="text-secondary">ImageNet Dataset: </strong>The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes.</p>
                            <p class="m-0 text-secondary" cite="https://towardsdatascience.com/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c"><strong>CNN: </strong>VGG-19 is focused on having convolution layers of 3x3 filter with a stride 1 and always used same padding and maxpool layer of 2x2 filter of stride 2. It follows this arrangement of convolution and max pool layers consistently throughout the whole architecture. In the end it has 2 FC(fully connected layers) followed by a softmax for output. The 16 in VGG16 refers to it has 16 layers that have weights. </p>
                            <a href="https://neurohive.io/en/popular-networks/vgg16/" alt="vgg-16"><p class="m-0">Learn More</p></a>
                        </div>                        
                    </div>                    
                </div>
                <h4 class="mt-4 a-h1">Data loading and Visualization</h4>
                <div class="row">                    
                    <div class="col-4">
                        <!-- <div class="mt-4"> -->
                            <h6 class="w-100 text-secondary">Folder Structure: </h6>
                            <ul>
                              <li>./Resources
                                <ul style="list-style-type:square;">
                                    <li>/Train
                                        <ul>
                                            <li>/WithMask
                                                <ul>
                                                    <li>/images..</li>
                                                  </ul>
                                            </li>
                                            <li>/WithoutMask
                                                <ul>
                                                    <li>/images..</li>
                                                  </ul>
                                            </li>
                                        </ul>
                                    </li>
                                </ul>
                              </li>
                            </ul>
                        <!-- </div>                             -->
                    </div>
                    <div class="col-4">
                        <h6 class="text-secondary">Dataframe:</h6>
                            <img src="./static/Images/dataframe.png" class="image-fluid w-100" >
                    </div>
                    <div class="col-4">
                        <img src="./static/Images/barchart.jpg" class="image-fluid w-100" >
                    </div>
                </div>   
                <h4 class="mt-4 a-h1">Data Augmentation</h4> 
                <div class="ml-4 mr-4"><p>Neural networks are heavily reliant on big data to avoid overfitting. Since we have limited data, we can solve this problem with <em>Data Augmentation</em>. Data augmentation is a technique to artificially create new training data from existing training data. It will increase the diversity of your training set by applying random (but realistic) transformations such as image rotation.</p> 
                 <p><strong>ImageDataGenerator: </strong>Dataset is augmneted using <em>ImageDataGenerator</em> that accepts the original data, randomly transforms it, and returns only the new, transformed data which will be used in training the model.</p>
                </div>
                <h4 class="mt-4 a-h1">Building and Training the Model</h4>  
                <div class="ml-4 mr-4">
                    <p>We built our VGG-19 model with Keras Sequential CNN model with various layers such as Flatten and Dense with activation <em>'softMax'</em>. We used the <em>'RMSProp'</em> optimizer and <em>‘binary_crossentropy’</em> as our loss function to address the two classes. </p>
                    <p><strong>Test Accuracy: </strong>After 33 epochs, our model achieved the test accuracy of 92%</p>
                    <img src="./static/Images/Test_accuracy.png" class="image-fluid w-50" >
                </div> 
                <h4 class="mt-4 a-h1">Model Accuracy and Loss</h4> 
                <div class="row">
                    <div class="col-12 mr-4">
                        <img src="./static/Images/model_loss.jpg" class="image-fluid w-100" >
                        <p class="mb-0"><strong>Note: </strong>As you can see model's Test accuracy is heigher than the training accuracy but there could be slight overfitting after epoch 23.</p>
                        <p><strong>Future Analysis: </strong>We would consider stopping and model from training using <em>TensorFlow Keras ModelCheckpoint</em>, which lets us save the best model while monitoring the value loss.</p>
                    </div>
                </div>
                <h4 class="mt-4 a-h1">OpenCV Face Recognition and Mask Prediction using the saved VGGNET model</h4> 
                <div>
                    <p><strong>Haar Cascade: </strong>HaarCascade is an object detection method used to locate an object of interest in images. We used OpenCV <em>.detectMultiScale</em> function with HaarCascade to generate a list of rectangles for all of the detected faces in the image as well as realtime video.</p>
                    <div class="row">
                        <div class="col-6">
                            <h4 class="a-h1">Face detection and Mask prediction of an Image</h4>
                            <img src="./static/Images/analysis_prediction.png" class="image-fluid w-100" >
                        </div>
                        <div class="col-6">
                            <h4 class="a-h1">Face detection and Mask prediction real-time</h4>
                            <img src="./static/Images/MaskNoMask.gif" class="image-fluid w-100">
                        </div>
                    </div>
                </div>
                <h4 class="mt-4 a-h1">Limitations: </h4>  
                <div class="ml-4 mr-4">
                    <!-- <p class="mb-0"><strong>OpenCV:</strong></p> -->
                            <p class="text-muted">Please consider the following before using the app:</p>
                            <ul>
                                <li class="mb-0"><em>Face Detection: </em>OpenCV wll not recongnize faces from a busy backgroud.</li>
                                <li class="mb-0"><em>Minimum Neighbours: </em>While our app can accurately predict for two neighbours, OpenCV might behave randmly with more than 2 neighbours.</li>
                                <li class="mb-0"><em>Face the camera: </em>OpenCV will not recongnize the face if the person is not facing the camera.</li>
                            </ul>
                   
                    
                    <br>
                    <!-- <p class="mb-0"><>Other: </strong></p> -->
                    <!-- <p class="mb-0"><strong>Browse Option:</strong>Browse option only works if the image uploaded is saved in UploadPic folder.</p> -->
                    <!-- <p>Limitations of Streaming
                        When the Flask application serves regular requests the request cycle is short. The web worker receives the request, invokes the handler function and finally returns the response. Once the response is sent back to the client the worker is free and ready to take on another request.
                        When a request that uses streaming is received, the worker remains attached to the client for the duration of the stream. When working with long, never ending streams such as a video stream from a camera, a worker will stay locked to the client until the client disconnects. This effectively means that unless specific measures are taken, the application can only serve as many clients as there are web workers. When working with the Flask application in debug mode that means just one, so you will not be able to connect a second browser window to watch the stream from two places at the same time.
                        There are ways to overcome this important limitation. The best solution in my opinion is to use a coroutine based web server such as gevent, which Flask fully supports. With the use of coroutines gevent is able to handle multiple clients on a single worker thread, as gevent modifies the Python I/O functions to issue context switches as necessary.</p> -->
                    <br>

                </div>         
                


                


            </div>
        </div>
    </div>

</section>


<!-- Javascript files -->
<!-- D3 JavaScript -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/5.5.0/d3.js"></script>

 <!--Our javaScript file-->
  <!---------------------------------------------->

</body>
</html>